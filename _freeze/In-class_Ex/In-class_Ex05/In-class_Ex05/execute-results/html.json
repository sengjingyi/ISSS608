{
  "hash": "c83014c26f37e1e8e1b07caf3f063d95",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"In-class Exercise 5\"\nauthor: \"Seng Jing Yi\"\ndate: \"May 11, 2024\"\ndate-modified: \"last-modified\"\nexecute: \n  eval: true\n  echo: true\n  warning: false\n  freeze: true\nformat: html\neditor: visual\n---\n\n\n# Exploring Mini Challenge 1\n\n## VAST Challenge 2024\n\nPackages:\n\n-   [Quanteda](https://quanteda.io/) - Quantitative Analysis of Textual Data\n\n-   [Readtext](https://readtext.quanteda.io/) - For reading text files in their various format.\n\n-   [Tidytext](https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html) - R package for text mining\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(tidyverse, readtext, quanteda, tidytext)\n```\n:::\n\n\nReading data where \"/\\*\" - Opening sub-directories within the data file and read ALL.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntext_data <- readtext(paste0(\"data/articles\", \"/*\"))\nview(text_data)\n```\n:::\n\n\nTokenising the article to identify key words (mainly nouns, excluding stop words)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nusenet_words <- text_data %>%\n  unnest_tokens(word,text) %>%\n  filter(str_detect(word, \"[a-z']$\"), \n         !word %in% stop_words$word)\n\nview(usenet_words)\n```\n:::\n\n\nCounting the most frequent word after tokenising\n\nConsider: Stem to get the root form of the word before counting\n\n\n::: {.cell}\n\n```{.r .cell-code}\nusenet_words %>%\n  count(word, sort = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nreadtext object consisting of 3260 documents and 0 docvars.\n# A data frame: 3,260 × 3\n  word             n text     \n  <chr>        <int> <chr>    \n1 fishing       2177 \"\\\"\\\"...\"\n2 sustainable   1525 \"\\\"\\\"...\"\n3 company       1036 \"\\\"\\\"...\"\n4 practices      838 \"\\\"\\\"...\"\n5 industry       715 \"\\\"\\\"...\"\n6 transactions   696 \"\\\"\\\"...\"\n# ℹ 3,254 more rows\n```\n\n\n:::\n:::\n\n\nBreaking down the text data with `tidyr` - Regex with `separate_wider_delim` (Link: <https://tidyr.tidyverse.org/reference/separate_wider_delim.html>)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntext_data_splitted <- text_data %>% \n  separate_wider_delim(\"doc_id\", \n                       delim = \"__0__\", \n                       names = c(\"X\", \"Y\"), \n                       too_few = \"align_end\")\n```\n:::\n\n\n## References for text handling:\n\n1.  Text mining with R - tidytext: <https://www.tidytextmining.com/>\n2.  Using `stringr` to split text: <https://stringr.tidyverse.org/>\n3.  Using `tidyr` to delimit text files: <https://tidyr.tidyverse.org/>\n\n# Handling Network Data\n\n## Loading json package\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(jsonlite, tidyverse, tidyr)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmc1_data <-fromJSON(\"data/mc1.json\")\n```\n:::\n\n\nData model: Multiple knowledge graph - Nodes and links (already in dataframe)\n\nClicking into the nodes and link, will be able to see the underlying data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Seeing the underlying data under nodes\nview(mc1_data[[\"nodes\"]])\nview(mc1_data[[\"links\"]])\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmc2_data <- fromJSON(\"data/mc2.json\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nview(mc2_data[[\"nodes\"]])\nview(mc2_data[[\"links\"]])\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Exporting for analysis\n\n#write_csv( mc1_data[[\"nodes\"]], \"mc1_nodes.csv\")\n#write_csv(mc1_data[[\"links\"]], \"mc1_link.csv\")\n#write_csv(mc2_data[[\"nodes\"]], \"mc2_nodes.csv\")\n#write_csv(mc2_data[[\"links\"]], \"mc2_links.csv\")\n```\n:::\n\n\n# Drawing graph with network data\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(jsonlite, tidygraph, ggraph, visNetwork, graphlayouts, ggforce, skimr, tidytext, tidyverse)\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}